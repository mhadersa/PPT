{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fd989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Path to the Hadoop configuration file (e.g., core-site.xml)\n",
    "config_file = '/path/to/hadoop/conf/core-site.xml'\n",
    "\n",
    "# Create a ConfigParser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config.read(config_file)\n",
    "\n",
    "# Display all sections and properties\n",
    "for section in config.sections():\n",
    "    print(f\"[{section}]\")\n",
    "    for key, value in config.items(section):\n",
    "        print(f\"{key} = {value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d21c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_total_file_size(hdfs_url, directory_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Get the list of files in the directory\n",
    "    file_list = client.list(directory_path, status=True)\n",
    "\n",
    "    # Calculate the total file size\n",
    "    total_size = sum(file['length'] for file in file_list)\n",
    "\n",
    "    # Convert the size to a human-readable format\n",
    "    total_size_formatted = client.status(directory_path, strict=False)['length']\n",
    "\n",
    "    return total_size, total_size_formatted\n",
    "\n",
    "# Example usage\n",
    "hdfs_url = \"http://localhost:50070\"\n",
    "directory_path = \"/user/example/directory\"\n",
    "\n",
    "total_size, total_size_formatted = calculate_total_file_size(hdfs_url, directory_path)\n",
    "print(\"Total File Size:\", total_size)\n",
    "print(\"Total File Size Formatted:\", total_size_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b3b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mrjob\n",
      "  Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.6/439.6 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\heman\\anaconda3\\lib\\site-packages (from mrjob) (6.0)\n",
      "Installing collected packages: mrjob\n",
      "Successfully installed mrjob-0.7.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def mapper_extract_words(self, _, line):\n",
    "        words = re.findall(r'\\w+', line.lower())\n",
    "        for word in words:\n",
    "            yield word, 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_n(self, _, word_count_pairs):\n",
    "        top_n = sorted(word_count_pairs, reverse=True)[:N]\n",
    "        for count, word in top_n:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_extract_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_n)\n",
    "        ]\n",
    "\n",
    "# Set the value of N for the top N most frequent words\n",
    "N = 10\n",
    "\n",
    "# Run the MapReduce job\n",
    "if __name__ == '__main__':\n",
    "    TopNWords.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ac111",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aed185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Hadoop cluster information\n",
    "namenode_host = 'namenode_hostname'\n",
    "namenode_port = '50070'\n",
    "datanode_port = '50075'\n",
    "\n",
    "# Check NameNode health\n",
    "namenode_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "namenode_response = requests.get(namenode_url).json()\n",
    "namenode_health = namenode_response['beans'][0]['State']\n",
    "\n",
    "# Check DataNode health\n",
    "datanode_url = f\"http://{namenode_host}:{datanode_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "datanode_response = requests.get(datanode_url).json()\n",
    "datanode_health = datanode_response['beans'][0]['State']\n",
    "\n",
    "# Print health status\n",
    "print(f\"NameNode Health: {namenode_health}\")\n",
    "print(f\"DataNode Health: {datanode_health}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # List files and directories in the specified HDFS path\n",
    "    file_list = client.list(hdfs_path, status=True)\n",
    "\n",
    "    # Print the file and directory names\n",
    "    for file in file_list:\n",
    "        file_name = file['path']\n",
    "        file_type = 'Directory' if file['type'] == 'DIRECTORY' else 'File'\n",
    "        print(f\"{file_type}: {file_name}\")\n",
    "\n",
    "# Example usage\n",
    "hdfs_url = \"http://localhost:50070\"\n",
    "hdfs_path = \"/user/example/directory\"\n",
    "\n",
    "list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efec64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_datanode_storage_utilization(namenode_host, namenode_port):\n",
    "    # Get the list of DataNodes from the Hadoop Namenode's JMX endpoint\n",
    "    namenode_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo\"\n",
    "    response = requests.get(namenode_url).json()\n",
    "    datanodes = response['beans'][0]['LiveNodes']\n",
    "\n",
    "    # Calculate storage utilization for each DataNode\n",
    "    storage_utilization = {}\n",
    "    for datanode in datanodes.values():\n",
    "        storage_utilization[datanode['name']] = {\n",
    "            'capacity': datanode['capacity'],\n",
    "            'used': datanode['used'],\n",
    "            'utilization': round(datanode['used'] / datanode['capacity'] * 100, 2)\n",
    "        }\n",
    "\n",
    "    return storage_utilization\n",
    "\n",
    "def analyze_storage_utilization(storage_utilization):\n",
    "    # Find the DataNode with the highest and lowest storage capacities\n",
    "    sorted_datanodes = sorted(storage_utilization.items(), key=lambda x: x[1]['capacity'])\n",
    "    highest_capacity_node = sorted_datanodes[-1]\n",
    "    lowest_capacity_node = sorted_datanodes[0]\n",
    "\n",
    "    return highest_capacity_node, lowest_capacity_node\n",
    "\n",
    "# Hadoop cluster information\n",
    "namenode_host = 'namenode_hostname'\n",
    "namenode_port = '50070'\n",
    "\n",
    "# Get storage utilization of DataNodes\n",
    "storage_utilization = get_datanode_storage_utilization(namenode_host, namenode_port)\n",
    "\n",
    "# Analyze storage utilization and find highest and lowest capacity nodes\n",
    "highest_capacity_node, lowest_capacity_node = analyze_storage_utilization(storage_utilization)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Highest Capacity Node: {highest_capacity_node[0]}\")\n",
    "print(f\"Capacity: {highest_capacity_node[1]['capacity']} bytes\")\n",
    "print(f\"Utilization: {highest_capacity_node[1]['utilization']}%\")\n",
    "print()\n",
    "print(f\"Lowest Capacity Node: {lowest_capacity_node[0]}\")\n",
    "print(f\"Capacity: {lowest_capacity_node[1]['capacity']} bytes\")\n",
    "print(f\"Utilization: {lowest_capacity_node[1]['utilization']}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe46826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# YARN ResourceManager information\n",
    "resourcemanager_host = 'resourcemanager_hostname'\n",
    "resourcemanager_port = '8088'\n",
    "\n",
    "# Submit a Hadoop job\n",
    "def submit_hadoop_job(job_name, jar_path, input_path, output_path):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"application\": {\n",
    "            \"application-name\": job_name,\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} input {input_path} output {output_path}\"\n",
    "                },\n",
    "                \"local-resources\": {\n",
    "                    \"resource\": [\n",
    "                        {\n",
    "                            \"name\": \"job.jar\",\n",
    "                            \"type\": \"FILE\",\n",
    "                            \"visibility\": \"APPLICATION\",\n",
    "                            \"uri\": jar_path,\n",
    "                            \"size\": -1\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"application-type\": \"MAPREDUCE\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    response_json = response.json()\n",
    "    return response_json['application-id']\n",
    "\n",
    "# Monitor job progress\n",
    "def monitor_job_progress(job_id):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{job_id}\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response_json = response.json()\n",
    "        app_state = response_json['app']['state']\n",
    "\n",
    "        if app_state == 'FINISHED' or app_state == 'FAILED' or app_state == 'KILLED':\n",
    "            break\n",
    "\n",
    "        print(f\"Job status: {app_state}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    return app_state\n",
    "\n",
    "# Retrieve final output\n",
    "def retrieve_output(output_path):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/webhdfs/v1{output_path}?op=OPEN\"\n",
    "    response = requests.get(url)\n",
    "    output = response.text\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "job_name = \"MyHadoopJob\"\n",
    "jar_path = \"hadoop_job.jar\"\n",
    "input_path = \"/user/example/input\"\n",
    "output_path = \"/user/example/output\"\n",
    "\n",
    "job_id = submit_hadoop_job(job_name, jar_path, input_path, output_path)\n",
    "print(f\"Job submitted with ID: {job_id}\")\n",
    "\n",
    "app_state = monitor_job_progress(job_id)\n",
    "print(f\"Job status: {app_state}\")\n",
    "\n",
    "if app_state == 'FINISHED':\n",
    "    final_output = retrieve_output(output_path)\n",
    "    print(\"Final Output:\")\n",
    "    print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# YARN ResourceManager information\n",
    "resourcemanager_host = 'resourcemanager_hostname'\n",
    "resourcemanager_port = '8088'\n",
    "\n",
    "# Submit a Hadoop job with resource requirements\n",
    "def submit_hadoop_job(job_name, jar_path, input_path, output_path, memory_mb, vcores):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"application\": {\n",
    "            \"application-name\": job_name,\n",
    "            \"am-container-spec\": {\n",
    "                \"commands\": {\n",
    "                    \"command\": f\"hadoop jar {jar_path} input {input_path} output {output_path}\"\n",
    "                },\n",
    "                \"local-resources\": {\n",
    "                    \"resource\": [\n",
    "                        {\n",
    "                            \"name\": \"job.jar\",\n",
    "                            \"type\": \"FILE\",\n",
    "                            \"visibility\": \"APPLICATION\",\n",
    "                            \"uri\": jar_path,\n",
    "                            \"size\": -1\n",
    "                        }\n",
    "                    ]\n",
    "                },\n",
    "                \"resource\": {\n",
    "                    \"memory\": memory_mb,\n",
    "                    \"vCores\": vcores\n",
    "                }\n",
    "            },\n",
    "            \"application-type\": \"MAPREDUCE\",\n",
    "            \"resource\": {\n",
    "                \"memory\": memory_mb,\n",
    "                \"vCores\": vcores\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    response_json = response.json()\n",
    "    return response_json['application-id']\n",
    "\n",
    "# Monitor job progress and track resource usage\n",
    "def monitor_job_progress(job_id):\n",
    "    url = f\"http://{resourcemanager_host}:{resourcemanager_port}/ws/v1/cluster/apps/{job_id}\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response_json = response.json()\n",
    "        app_state = response_json['app']['state']\n",
    "        app_memory = response_json['app']['allocatedMB']\n",
    "        app_vcores = response_json['app']['allocatedVCores']\n",
    "\n",
    "        if app_state == 'FINISHED' or app_state == 'FAILED' or app_state == 'KILLED':\n",
    "            break\n",
    "\n",
    "        print(f\"Job status: {app_state}\")\n",
    "        print(f\"Allocated Memory: {app_memory} MB\")\n",
    "        print(f\"Allocated vCores: {app_vcores}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    return app_state\n",
    "\n",
    "# Example usage\n",
    "job_name = \"MyHadoopJob\"\n",
    "jar_path = \"hadoop_job.jar\"\n",
    "input_path = \"/user/example/input\"\n",
    "output_path = \"/user/example/output\"\n",
    "memory_mb = 1024\n",
    "vcores = 2\n",
    "\n",
    "job_id = submit_hadoop_job(job_name, jar_path, input_path, output_path, memory_mb, vcores)\n",
    "print(f\"Job submitted with ID: {job_id}\")\n",
    "\n",
    "app_state = monitor_job_progress(job_id)\n",
    "print(f\"Job status: {app_state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "import time\n",
    "\n",
    "class PerformanceComparisonJob(MRJob):\n",
    "\n",
    "    def configure_args(self):\n",
    "        super(PerformanceComparisonJob, self).configure_args()\n",
    "        self.add_passthru_arg('--split-size', type=int, default=1000,\n",
    "                              help='Input split size')\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield None, line\n",
    "\n",
    "    def reducer(self, _, lines):\n",
    "        time.sleep(0.1)  # Simulate processing time\n",
    "        yield None, None\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            self.mr(mapper=self.mapper, reducer=self.reducer)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    performance_job = PerformanceComparisonJob()\n",
    "    start_time = time.time()\n",
    "    performance_job.run_job()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution Time: {execution_time} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
