{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80688488",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDDOperationsExample\")\n",
    "\n",
    "# Create an RDD from a local data source\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Transformations and Actions\n",
    "\n",
    "# Map transformation: Square each element\n",
    "squared_rdd = rdd.map(lambda x: x * x)\n",
    "\n",
    "# Filter transformation: Select only even numbers\n",
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Reduce action: Calculate the sum of all elements\n",
    "sum_of_elements = rdd.reduce(lambda x, y: x + y)\n",
    "\n",
    "# Aggregate action: Calculate the sum and count of elements\n",
    "sum_count = rdd.aggregate((0, 0),\n",
    "                          lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "                          lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "\n",
    "# Print the results\n",
    "print(\"Original RDD:\")\n",
    "for element in rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "print(\"Squared RDD:\")\n",
    "for element in squared_rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "print(\"Even RDD:\")\n",
    "for element in even_rdd.collect():\n",
    "    print(element)\n",
    "\n",
    "print(\"Sum of Elements:\", sum_of_elements)\n",
    "print(\"Sum and Count:\", sum_count)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_file_path = \"path_to_your_csv_file.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Filtering\n",
    "filtered_df = df.filter(col(\"age\") > 30)\n",
    "\n",
    "# Grouping\n",
    "grouped_df = df.groupBy(\"gender\").count()\n",
    "\n",
    "# Joining\n",
    "other_data_path = \"path_to_other_data_file.csv\"\n",
    "other_data_df = spark.read.csv(other_data_path, header=True, inferSchema=True)\n",
    "joined_df = df.join(other_data_df, \"id\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Filtered DataFrame:\")\n",
    "filtered_df.show()\n",
    "\n",
    "print(\"Grouped DataFrame:\")\n",
    "grouped_df.show()\n",
    "\n",
    "print(\"Joined DataFrame:\")\n",
    "joined_df.show()\n",
    "\n",
    "# Register the DataFrame as a temporary view for Spark SQL\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "\n",
    "# Apply Spark SQL queries\n",
    "sql_query = \"SELECT gender, AVG(age) AS average_age FROM my_table GROUP BY gender\"\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "print(\"Result of Spark SQL Query:\")\n",
    "result_df.show()\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local[2]\", \"SparkStreamingExample\")\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Configure the StreamingContext to consume data from a socket\n",
    "hostname = \"localhost\"\n",
    "port = 9999\n",
    "lines = ssc.socketTextStream(hostname, port)\n",
    "\n",
    "# Perform streaming transformations and actions\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Print the word counts\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4cb6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLDataSources\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Connect Spark with a relational database\n",
    "database_url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
    "database_properties = {\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Read data from the database using Spark SQL\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", database_url) \\\n",
    "    .option(\"dbtable\", \"your_table_name\") \\\n",
    "    .option(\"user\", database_properties[\"user\"]) \\\n",
    "    .option(\"password\", database_properties[\"password\"]) \\\n",
    "    .option(\"driver\", database_properties[\"driver\"]) \\\n",
    "    .load()\n",
    "\n",
    "# Perform SQL operations on the DataFrame\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "result = spark.sql(\"SELECT * FROM my_table WHERE age > 30\")\n",
    "\n",
    "# Print the result\n",
    "result.show()\n",
    "\n",
    "# Explore integration capabilities with other data sources\n",
    "hdfs_file_path = \"hdfs://localhost:9000/path/to/your/file.csv\"\n",
    "s3_file_path = \"s3a://your_bucket/path/to/your/file.csv\"\n",
    "\n",
    "# Read data from HDFS\n",
    "hdfs_df = spark.read.csv(hdfs_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Read data from Amazon S3\n",
    "s3_df = spark.read.csv(s3_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Perform operations on the DataFrames\n",
    "# ...\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
